VideoEncoder: {
    "patch_size": [1,8,8],
    "embed_dim": 128,
    "depths":[2, 2, 14, 2, 2, 2],
    "downsample_stages":[0, 1, 4],
    "stages":[0, 1, 2, 2, 2, 3],
    "num_heads":[4, 8, 16, 16, 16, 32],
    "window_size":[[2,3,5],[4,3,5],[8,3,5],[16,3,5],[16,3,5],[32,3,5]], #time, h, w
    "patch_norm": True,
    "local_window": 8
}


bert_config: "src/configs/bert_large_config.json"
stage: 1
type_vocab_size: 8
num_local_layers: 8
stage1_layers: 12
bert_frozen_stage: -1

log_tb: true


WEIGHTS: 
    model_weight: ''
    stage1_model_weight: ''
    bert_weight: 'project/lfvila/pretrained/bert-large-uncased/pytorch_model.bin'
    swin_weight: 'project/lfvila/pretrained/swin/swin_base_patch4_window12_384_22k.pth'
    pretrained_2d: True

DATA:
    use_lmdb_train_data: True
    len_lmdb_train_data: 8523237 
    BATCH_SIZE_per_gpu: 16
    NUM_WORKERS: 12
    PIN_MEMORY: True

    sample_frame: 32
    sample_clip: 4
    input_res: [192, 320]
    center_crop: 200

    DATASET_train: {
            'name': 'PreTrainDataset-train',
            'type': 'PreTrainDataset',
            'metadata_dir': 'datasets/lfvila_data/pretrain/train_db',
            'video_path': 'datasets/hdvila100m/video_clip_3fps'
        }

    DATASET_val: [{
            'name': 'RetrievalDataset-val',
            'type': 'RetrievalDataset',
            'metadata_dir': 'datasets/lfvila_data/task/actnet/val_s.jsonl',
            'video_path': 'datasets/activitynet/actnet_video'
        },
        {
            'name': 'PreTrainDataset-val',
            'type': 'PreTrainDataset',
            'metadata_dir': 'datasets/lfvila_data/pretrain/val.jsonl',
            'video_path': 'datasets/hdvila100m/video_clip_3fps'
        }   
        ]


TRAINING:
    BREAK_STEP: 10000000000
    EPOCHS: 10
    WARMUP_EPOCHS: 1
    WARMUP_LR: 0.
    LR_SCHEDULER: {
        'NAME': 'linear',
        'DECAY_EPOCHS': 10,
        }

    use_mlm: false

    ct_global_loss_weight: 1


    use_time_match: true
    ct_time_loss_weight: 0.25
    num_key: 2
    num_value: 2
    num_other_neg: 3
    time_temp: 0.05
    use_mask_equal: false


    temp: 0.05
    weight_decay: 0.05

    save_dir: "project/lfvila/lfvila_save/pretrain_stage1"
    checkpoint_step: 4000
    save_step: 2000
    print_step: 100
    eval_step: 500

deepspeed_config: {
    "train_micro_batch_size_per_gpu": 16,
    "gradient_accumulation_steps": 1,
    "steps_per_print": 500,


    "zero_optimization": {
      "stage": 2,
      "allgather_partitions": true,
      "allgather_bucket_size": 5.0e+8,
      "overlap_comm": false,
      "reduce_scatter": true,
      "reduce_bucket_size": 5.0e+8,
      "contiguous_gradients" : false,
      "stage3_gather_fp16_weights_on_model_save": true
    },

    "fp16": {
      "enabled": true,
      "loss_scale": 0,
      "loss_scale_window": 1000,
      "initial_scale_power": 32,
      "hysteresis": 2,
      "min_loss_scale": 1
  },

    "optimizer": {
        "type": "AdamW",
        "params": {
        "lr": 5.0e-5,
        "betas": [0.9, 0.98],
        "eps": 1.0e-8,
        "weight_decay": 5.0e-2
        }
    },


    "sparse_attention": {
      "mode": "fixed",
      "block": 32,
      "different_layout_per_head": true,
      "num_local_blocks": 16,
      "num_global_blocks": 1,
      "attention": "bidirectional",
      "horizontal_global_attention": true,
      "num_different_global_patterns": 4
    }
}


  


  
